{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I965pXSHOX5"
      },
      "source": [
        "# Twitter Emotion Classification\n",
        "\n",
        "In this project we will identify the primary emotion expressed in a tweet. We will be using SMILE dataset for this task. The project will be tested using 2 methodologies. The first method used for this task is Rule-based method, the second method is ML based Random Forest and the thrid method is using DistilBERT Transformer.\n",
        "\n",
        "#### Random Forest:\n",
        "\n",
        "Random Forest is an ensemble ML model that uses multiple decision trees algorithm at the same time during training . For classification task, the ouput class chosen by majority of decision trees will be selected as the final output class for the random forest. We choose this model for testing because Random Forest gives comparatively accurate and faster results than other algorithms as it grows multiple decision trees for giving output. It also works well while using larger datasets and it is capable of maintaining accuracy even if a large proportion of dataset is missing.  \n",
        "\n",
        "#### DistilBERT Transformer:\n",
        "\n",
        "BERT (Bi-Directional Encoder Representation Transformer) is a transformer model in NLP which calculates embeddings from the dataset that can be used for achieving various other NLP based tasks. **DistilBERT** is a derivative of BERT that reduces the model size upto 40% eventually giving faster results. Transformers are used in ML to reduce the training time as they enable simultaneous sequence processing through parallelization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omKQipjPFz7s"
      },
      "source": [
        "# Installing required libraries\n",
        "\n",
        "\n",
        "In the below cell, first we will be downloading dataset from an opensource website using wget. Along with that we will install emoji library to extract the meaning of emojis used in the tweets. Along with that we will install nltk library that provides a wide range of suite of programs for symbolic ans statistical NLP for English language. The punkt module in nltk is an unsupervised training model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_PIJoH4HVqu",
        "outputId": "8939280c-9832-45c2-fd2a-05eb29c749d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-22 22:57:43--  https://figshare.com/ndownloader/files/4988956\n",
            "Resolving figshare.com (figshare.com)... 52.215.42.80, 54.217.150.101, 2a05:d018:1f4:d000:646e:611b:a755:ea07, ...\n",
            "Connecting to figshare.com (figshare.com)|52.215.42.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/4988956/smileannotationsfinal.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230922/eu-west-1/s3/aws4_request&X-Amz-Date=20230922T225744Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=3c00c03eeea546c4038f146a60f6eb1ec109fe5cd5eacca36a2e19cd08b1298f [following]\n",
            "--2023-09-22 22:57:44--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/4988956/smileannotationsfinal.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230922/eu-west-1/s3/aws4_request&X-Amz-Date=20230922T225744Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=3c00c03eeea546c4038f146a60f6eb1ec109fe5cd5eacca36a2e19cd08b1298f\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.36.130, 52.218.120.56, 52.218.100.171, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.130|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 429669 (420K) [binary/octet-stream]\n",
            "Saving to: â€˜data.csvâ€™\n",
            "\n",
            "data.csv            100%[===================>] 419.60K   734KB/s    in 0.6s    \n",
            "\n",
            "2023-09-22 22:57:45 (734 KB/s) - â€˜data.csvâ€™ saved [429669/429669]\n",
            "\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.8.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!wget -O data.csv \"https://figshare.com/ndownloader/files/4988956\"\n",
        "!pip install emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMEwTiYHZ3J"
      },
      "source": [
        "## Task 1. Data Cleaning, Preprocessing, and splitting\n",
        "The `data` environment contains the SMILE dataset loaded into a pandas dataframe object. Our dataset has three columns: id, tweet, and label. The `tweet` column contains the raw scraped tweet and the `label` column contains the annotated emotion category. Each tweet is labelled with one of the following emotion labels:\n",
        "- 'nocode', 'not-relevant'\n",
        "- 'happy', 'happy|surprise', 'happy|sad'\n",
        "- 'angry', 'disgust|angry', 'disgust'\n",
        "- 'sad', 'sad|disgust', 'sad|disgust|angry'\n",
        "- 'surprise'\n",
        "\n",
        "### Task 1a. Label Consolidation\n",
        "As we can see above the annotated categories are complex. Several tweets express complex emotions like (e.g. 'happy|sad') or multiple emotions (e.g. 'sad|disgust|angry'). The first things we need to do is clean up our dataset by removing complex examples and consolidating others so that we have a clean set of emotions to predict.\n",
        "\n",
        "For Task 1a., we will write a code which do the following:\n",
        "1. Drops all rows which have the label \"happy|sad\", \"happy|surprise\", 'sad|disgust|angry', and 'sad|angry'.\n",
        "2. Re-label 'nocode' and 'not-relevant' as 'no-emotion'.\n",
        "3. Re-label 'disgust|angry' and 'disgust' as 'angry'.\n",
        "4. Re-label 'sad|disgust' as 'sad'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKSm3ZcNHabO",
        "outputId": "677fb472-fc22-4999-9f01-f0df1baa8653"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"https://figshare.com/ndownloader/files/4988956\", names=['id', 'tweet', 'label'])\n",
        "\n",
        "data = data.loc[~data.label.isin([\"happy|sad\", \"happy|surprise\", \"sad|disgust|angry\", \"sad|angry\"])]\n",
        "\n",
        "data.loc[(data.label == 'not-relevant'), 'label']='no-emotion'\n",
        "data.loc[(data.label == 'nocode'), 'label']='no-emotion'\n",
        "data.loc[(data.label == 'disgust|angry'), 'label']='angry'\n",
        "data.loc[(data.label == 'disgust'), 'label']='angry'\n",
        "data.loc[(data.label == 'sad|disgust'), 'label']='sad'\n",
        "data.loc[(data.label == 'sad'), 'label']='sad'\n",
        "\n",
        "type(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg-m3lROHex9"
      },
      "source": [
        "### Task 1b. Tweet Cleaning and Processing\n",
        "Raw tweets are noisy. Consider the example below:\n",
        "```\n",
        "'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up #foundationcourses. ðŸ˜ '\n",
        "```\n",
        "The mention @tateliverpool and hashtag #BobandRoberta are extra noise that don't directly help with understanding the emotion of the text. The accompanying emoji can be useful but needs to be decoded to it text form :angry: first.\n",
        "\n",
        "For this task we will perform the following preprocessing steps:\n",
        "1. Lower case all text\n",
        "2. De-emoji the text\n",
        "3. Remove all hashtags, mentions, and urls\n",
        "4. Remove all non-alphabet characters except the followng punctuations: period, exclamation mark, and question mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJHQXAizHjoE",
        "outputId": "d8482a10-b908-4b20-c620-0bba53c7695d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am angry more artists that have a profile are not speaking up! angryface\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "import re\n",
        "\n",
        "def preprocess_tweet(tweet: str) -> str:\n",
        "    # Lower case the tweet\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    #remove emoji\n",
        "    tweet = emoji.demojize(tweet)\n",
        "\n",
        "    # Remove all hashtags, mentions, and URLs\n",
        "    tweet = re.sub(r'\\#\\w+|\\@\\w+|https?://\\S+', '', tweet)\n",
        "\n",
        "    # Remove all non-alphabet characters except period, exclamation mark, and question mark\n",
        "    tweet = re.sub(r'[^a-z\\s.!?\\s+]+', '', tweet)\n",
        "\n",
        "    # Remove extra blank spaces from the test\n",
        "    tweet = re.sub('\\s+', ' ', tweet).strip()\n",
        "\n",
        "    return tweet\n",
        "\n",
        "\n",
        "test_tweet = \"'@tateliverpool #BobandRoberta: I am angry more artists that have a profile are not speaking up! #foundationcourses ðŸ˜ '\"\n",
        "print(preprocess_tweet(test_tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zb71pAhcHodX"
      },
      "outputs": [],
      "source": [
        "# Create new column with cleaned tweets. We will use this for the subsequent tasks\n",
        "data[\"cleaned_tweet\"] = data[\"tweet\"].apply(preprocess_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoBlBQQFHpmG"
      },
      "source": [
        "### Task 1c. Generating Evaluation Splits\n",
        "Finally, we need to split our data into a train, validation, and test set. We will split the data using a 60-20-20 split, where 60% of our data is used for training, 20% for validation, and 20% for testing. As the dataset is heaviliy imbalanced, we will use stratify parameter to ensure that the label distributions across the three splits are roughly equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dgc7zu0gHqm6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Your code here\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.4, stratify=data['label'], random_state=2023)\n",
        "test, val = train_test_split(test, test_size=0.2, stratify=test['label'], random_state=2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l_-J8_jjHSU",
        "outputId": "9cc3872b-8f1c-4122-db7b-4affa4e5f72a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAu45L3QHutm"
      },
      "source": [
        "## Task 2: Naive Baseline Using a Rule-based Classifier\n",
        "\n",
        "Now that we have a dataset, let's work on developing some solutions for emotion classification. We'll start with implementing a simple rule-based classifier which will also serve as our naive baseline. Emotive language (e.g. awesome, feel great, super happy) can be a strong signal as to the overall emotion being by the tweet. For each emotion in our label space (happy, surprised, sad, angry) we will generate a set of words and phrases that are often associated with that emotion. At classification time, the classifier will calculate a score based on the overlap between the words in the tweet and the emotive words and phrases for each of the emotions. The emotion label with the highest overlap will be selected as the prediction and if there is no match the \"no-emotion\" label will be predicted. We can break the implementation of this rules-based classifier into three steps:\n",
        "1. Emotive language extraction from train examples\n",
        "2. Developing a scoring algorithm\n",
        "3. Building the end-to-end classification flow\n",
        "\n",
        "### Task 2a. Emotive Language Extraction\n",
        "For this task we will generate a set of unigrams and bigrams that will be used to predict each of the labels. Using the training data we will need to extract all the unique unigrams and bigrams associated with each label (excluding no-emotion). Then we should ensure that the extracted terms for each emotion label do not appear in the other lists. In the real world, we would then manually curate the generated lists to ensure that associated words were useful and emotive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ozjSEbagHvmW"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Function to extract unigrams and bigrams from examples\n",
        "def extract_words(examples: List[str]):\n",
        "    \"\"\"\n",
        "    Given a list of tweets, return back the unigrams and bigrams found\n",
        "    across all the tweets.\n",
        "    \"\"\"\n",
        "    word_set = set()\n",
        "    for example in examples:\n",
        "      # print(example)\n",
        "      unigrams = list(ngrams(example.split(), n=1))\n",
        "      bigrams = list(ngrams(example.split(), n=2))\n",
        "      word_set.update(unigrams)\n",
        "      word_set.update(bigrams)\n",
        "    return word_set\n",
        "\n",
        "# Extract unique unigrams and bigrams for each label\n",
        "happy_words = extract_words(train[train[\"label\"] == \"happy\"][\"cleaned_tweet\"].values.tolist())\n",
        "surprise_words = extract_words(train[train[\"label\"] == \"surprise\"][\"cleaned_tweet\"].values.tolist())\n",
        "sad_words = extract_words(train[train[\"label\"] == \"sad\"][\"cleaned_tweet\"].values.tolist())\n",
        "angry_words = extract_words(train[train[\"label\"] == \"angry\"][\"cleaned_tweet\"].values.tolist())\n",
        "\n",
        "# print(happy_words)\n",
        "\n",
        "\n",
        "# Remove any words that appear in more than one list\n",
        "\n",
        "happy_words = happy_words.difference(surprise_words, sad_words, angry_words)\n",
        "surprise_words = surprise_words.difference(happy_words, sad_words, angry_words)\n",
        "sad_words = sad_words.difference(surprise_words, happy_words, angry_words)\n",
        "angry_words = angry_words.difference(surprise_words, sad_words, happy_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ6vlgcHH0-G"
      },
      "source": [
        "### Task 2b. Scoring using set overlaps\n",
        "\n",
        "Next we will implement to scoring algorithm. Our score will simply be the count of overlapping terms between tweet text and emotive terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UN4E5de4H1zm"
      },
      "outputs": [],
      "source": [
        "def score_tweet(tweet: str, emotive_words: List[str]) -> int:\n",
        "    tweet_words = set(tweet.split())\n",
        "    emotive_words_set = set(emotive_words)\n",
        "    score = len(tweet_words & emotive_words_set)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6qORzgxH3X4"
      },
      "source": [
        "### 2c. Rule-based classification\n",
        "Let put together our rules-based classfication system. Given a tweet, `simple_clf` will generate the overlap score\n",
        "for each of emotion labels and return the emotion label with the highest score. If there is no match amongst the emotions, the classifier will return 'no-emotion'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E8IEuXP0H46K"
      },
      "outputs": [],
      "source": [
        "def simple_clf(tweet: str) -> str:\n",
        "    \"\"\"\n",
        "    Given a tweet, calculate all the emotion overlap scores.\n",
        "    Return the emotion label which has the largest score. If\n",
        "    overlap score is 0, return no-emotion.\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "    happy_score = score_tweet(tweet, happy_words)\n",
        "    surprise_score = score_tweet(tweet, surprise_words)\n",
        "    sad_score = score_tweet(tweet, sad_words)\n",
        "    angry_score = score_tweet(tweet, angry_words)\n",
        "\n",
        "    scores = [('happy', happy_score), ('surprise', surprise_score), ('sad', sad_score), ('angry', angry_score)]\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if scores[0][1] == 0:\n",
        "        return 'no-emotion'\n",
        "    else:\n",
        "        return scores[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YbxbkhtH-nC"
      },
      "source": [
        "After finishing the above section, let's evaluate how our model did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUE4GNIbH8dp",
        "outputId": "a6d59a58-fbd9-44db-d1d3-78e2c0cf8b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.00      0.00      0.00        23\n",
            "       happy       0.00      0.00      0.00       364\n",
            "  no-emotion       0.58      1.00      0.74       571\n",
            "         sad       0.00      0.00      0.00        11\n",
            "    surprise       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.58       980\n",
            "   macro avg       0.12      0.20      0.15       980\n",
            "weighted avg       0.34      0.58      0.43       980\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "preds = test[\"cleaned_tweet\"].apply(simple_clf)\n",
        "print(classification_report(test[\"label\"], preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jlp3Qx1IC68"
      },
      "source": [
        "## Task 3. Machine learning w/ grammar augmented features\n",
        "\n",
        "Now that we have a naive baseline, let's build a more sophisticated solution using machine learning. Up to this point, we have only considered the words in the tweet as our primary features. The rules-based approach is a very simple bag-of-words classifier. Can we improve performance if we provide some additional linguistic knowledge?\n",
        "\n",
        "For Task 3 we will do the following:\n",
        "- Generate part-of-speech features our tweets\n",
        "- Train two different machine learning classifiers, one with linguistic features and one without\n",
        "- Evaluate the trained models on the test set\n",
        "\n",
        "### Task 3a. Grammar Augmented Feature Generation\n",
        "For this task, we will be generating part-of-speech tags for each token in our tweet. Additionally we'll lemmatize the text as well. We will directly include the POS information by appending the tag to the lemma of word itself. For example:\n",
        "```\n",
        "Raw Tweet: I am very angry with the increased prices.\n",
        "POS Augmented Tweet: I-PRP be-VBP very-RB angry-JJ with-IN the-DT increase-VBN price-NNS .-.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JAQ7V7IGIFCz",
        "outputId": "a1b653e3-d7d4-442c-db71-7d8a0ecc70cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I-PRP be-VBP very-RB angry-JJ with-IN the-DT increase-VBN price-NNS .-.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def generate_pos_features(tweet: str) -> str:\n",
        "    \"\"\"\n",
        "    Given a tweet, return the lemmatized tweet augmented\n",
        "    with POS tags.\n",
        "    E.g.:\n",
        "    Input: \"cats are super cool.\"\n",
        "    output: \"cat-NNS be-VBP super-RB cool-JJ .-.\"\n",
        "    \"\"\"\n",
        "\n",
        "    doc = nlp(tweet)\n",
        "    pos_features = []\n",
        "    for token in doc:\n",
        "        lemma = token.lemma_\n",
        "        pos = token.tag_\n",
        "        pos_features.append(f\"{lemma}-{pos}\")\n",
        "\n",
        "    return \" \".join(pos_features)\n",
        "\n",
        "sample_tweet = \"I am very angry with the increased prices.\"\n",
        "generate_pos_features(sample_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H5N5gJCCIJ3z"
      },
      "outputs": [],
      "source": [
        "train[\"tweet_with_pos\"] = train[\"cleaned_tweet\"].apply(generate_pos_features)\n",
        "test[\"tweet_with_pos\"] = test[\"cleaned_tweet\"].apply(generate_pos_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAoYl4h0IOgb"
      },
      "source": [
        "### Task 3b. Model Training\n",
        "Next we will train two seperate RandomForest Classifier models. For this task you will generate two sets of input features using the `TfidfVectorizer`. We generate Tfidf statistic on the`cleaned_tweet` and the `tweet_with_pos` columns.\n",
        "\n",
        "Once we've generated features, we will train two different Random Forest classifiers with the generated features and generate the predictions on the test set for each classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cE2NgOBnIQPg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create TfidfVectorizer instances for cleaned_tweet and tweet_with_pos\n",
        "cleaned_tweet_vectorizer = TfidfVectorizer()\n",
        "pos_tweet_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate features for cleaned_tweet\n",
        "cleaned_tweet_train_features = cleaned_tweet_vectorizer.fit_transform(train[\"cleaned_tweet\"])\n",
        "cleaned_tweet_test_features = cleaned_tweet_vectorizer.transform(test[\"cleaned_tweet\"])\n",
        "\n",
        "# Generate features for tweet_with_pos\n",
        "pos_tweet_train_features = pos_tweet_vectorizer.fit_transform(train[\"tweet_with_pos\"])\n",
        "pos_tweet_test_features = pos_tweet_vectorizer.transform(test[\"tweet_with_pos\"])\n",
        "\n",
        "# Train Random Forest Classifier on cleaned_tweet features\n",
        "cleaned_tweet_clf = RandomForestClassifier(random_state=42)\n",
        "cleaned_tweet_clf.fit(cleaned_tweet_train_features, train[\"label\"])\n",
        "\n",
        "# Train Random Forest Classifier on tweet_with_pos features\n",
        "pos_tweet_clf = RandomForestClassifier(random_state=42)\n",
        "pos_tweet_clf.fit(pos_tweet_train_features, train[\"label\"])\n",
        "\n",
        "# Generate predictions on the test set for each classifier\n",
        "cleaned_tweet_predictions = cleaned_tweet_clf.predict(cleaned_tweet_test_features)\n",
        "pos_tweet_predictions = pos_tweet_clf.predict(pos_tweet_test_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUL5ZI9wITW3"
      },
      "source": [
        "### Task 3c.\n",
        "Generating classification reports for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B8ep5lhIUiT",
        "outputId": "8fe104cd-ddc4-4e86-b848-fffeacfb2346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report for TFIDF features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.33      0.09      0.14        23\n",
            "       happy       0.81      0.72      0.76       364\n",
            "  no-emotion       0.79      0.89      0.84       571\n",
            "         sad       0.00      0.00      0.00        11\n",
            "    surprise       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.79       980\n",
            "   macro avg       0.39      0.34      0.35       980\n",
            "weighted avg       0.77      0.79      0.77       980\n",
            "\n",
            "Classification report for TFIDF w/ POS features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.33      0.09      0.14        23\n",
            "       happy       0.77      0.71      0.74       364\n",
            "  no-emotion       0.78      0.87      0.82       571\n",
            "         sad       0.00      0.00      0.00        11\n",
            "    surprise       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.77       980\n",
            "   macro avg       0.38      0.33      0.34       980\n",
            "weighted avg       0.75      0.77      0.76       980\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Classification Report for Tfidf features\n",
        "print(\"Classification report for TFIDF features\")\n",
        "# Your code here\n",
        "\n",
        "print(classification_report(test[\"label\"], cleaned_tweet_predictions))\n",
        "\n",
        "# Classfication Report for POS features\n",
        "print(\"Classification report for TFIDF w/ POS features\")\n",
        "# Your code here\n",
        "\n",
        "print(classification_report(test[\"label\"], pos_tweet_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n015Byn3IWhW"
      },
      "source": [
        "### Evaluating Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXxLPauyOt7"
      },
      "source": [
        "Looking at the accuracy score we can conclude that both the model performance was very similar with accuracy score for TFIDF features (0.79) being slightly better than accuracy score of TFIDF features with POS Tagging (0.76). Both models have higher precision, recall, and f1 score for no-emotion category. After that, the second best scores generated were for Happy emotion. Angry emotion didn't have good results with sad emotions at 0 results being the worst emotion to detect. Here, general TFIDF model performed slightly better than model with POS tagging but with good quality data TFIDF POS tagging model can outperform general TFIDF model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKY4ti3_IYvA"
      },
      "source": [
        "## Task 4. Transfer Learning with DistilBERT\n",
        "\n",
        "For this task we will finetune a pretrained language model (DistilBERT) using the huggingface `transformers` library. For this task we will need to:\n",
        "- Encode the tweets using the BERT tokenizer\n",
        "- Create pytorch datasets for for the train, val and test datasets\n",
        "- Finetune the distilbert model for 5 epochs\n",
        "- Extract predictions from the model's output logits and convert them into the emotion labels.\n",
        "- Generate a classification report on the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vHzUBl7dIXgx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] >> NULL\n",
        "!pip install accelerate -U >> NULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LTh8YJsdImOC",
        "outputId": "e197ab2f-902e-40fc-80cf-be6bdbc3e1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer output a dictionary: {'input_ids': [101, 1996, 4248, 2829, 4419, 5598, 2058, 1996, 13971, 3899, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] the quick brown fox jumped over the lazy dog. [SEP]\n",
            "{'input_ids': tensor([[  101,  2197,  2733,  ...,  4371,  1035,   102],\n",
            "        [  101,  2156,  2035,  ...,  8299,  1024,   102],\n",
            "        [  101,  1012,  2129,  ...,  1012,  2522,   102],\n",
            "        ...,\n",
            "        [  101,  2202,  1001,  ...,  1010, 11204,   102],\n",
            "        [  101,  2054,  1037,  ..., 22949,  4939,   102],\n",
            "        [  101, 14534,  2509,  ...,  1037,  2866,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
            "{'input_ids': tensor([[  101,  2197,  2733,  1024,  1001,  5033,  2696,  2100,  1030, 13970,\n",
            "          7946, 11106,  4313,  1030,  9902,  3669,  6299, 16869,  1030, 10424,\n",
            "          2666,  4371,  1035,   102],\n",
            "        [  101,  2156,  2035,  1996,  7760,  2013,  9317,  1005,  1055,  1001,\n",
            "          6324,  3207, 20110,  3258,  2724,  2012,  1030,  8223,  7606, 14820,\n",
            "          1024,  8299,  1024,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([2, 2])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_embeddings.weight False\n",
            "position_embeddings.weight False\n",
            "LayerNorm.weight False\n",
            "LayerNorm.bias False\n",
            "1.attention.q_lin.weight False\n",
            "1.attention.q_lin.bias False\n",
            "1.attention.k_lin.weight False\n",
            "1.attention.k_lin.bias False\n",
            "1.attention.v_lin.weight False\n",
            "1.attention.v_lin.bias False\n",
            "1.attention.out_lin.weight False\n",
            "1.attention.out_lin.bias False\n",
            "1.sa_layer_norm.weight False\n",
            "1.sa_layer_norm.bias False\n",
            "1.ffn.lin1.weight False\n",
            "1.ffn.lin1.bias False\n",
            "1.ffn.lin2.weight False\n",
            "1.ffn.lin2.bias False\n",
            "1.output_layer_norm.weight False\n",
            "1.output_layer_norm.bias False\n",
            "2.attention.q_lin.weight False\n",
            "2.attention.q_lin.bias False\n",
            "2.attention.k_lin.weight False\n",
            "2.attention.k_lin.bias False\n",
            "2.attention.v_lin.weight False\n",
            "2.attention.v_lin.bias False\n",
            "2.attention.out_lin.weight False\n",
            "2.attention.out_lin.bias False\n",
            "2.sa_layer_norm.weight False\n",
            "2.sa_layer_norm.bias False\n",
            "2.ffn.lin1.weight False\n",
            "2.ffn.lin1.bias False\n",
            "2.ffn.lin2.weight False\n",
            "2.ffn.lin2.bias False\n",
            "2.output_layer_norm.weight False\n",
            "2.output_layer_norm.bias False\n",
            "3.attention.q_lin.weight False\n",
            "3.attention.q_lin.bias False\n",
            "3.attention.k_lin.weight False\n",
            "3.attention.k_lin.bias False\n",
            "3.attention.v_lin.weight False\n",
            "3.attention.v_lin.bias False\n",
            "3.attention.out_lin.weight False\n",
            "3.attention.out_lin.bias False\n",
            "3.sa_layer_norm.weight False\n",
            "3.sa_layer_norm.bias False\n",
            "3.ffn.lin1.weight False\n",
            "3.ffn.lin1.bias False\n",
            "3.ffn.lin2.weight False\n",
            "3.ffn.lin2.bias False\n",
            "3.output_layer_norm.weight False\n",
            "3.output_layer_norm.bias False\n",
            "4.attention.q_lin.weight False\n",
            "4.attention.q_lin.bias False\n",
            "4.attention.k_lin.weight False\n",
            "4.attention.k_lin.bias False\n",
            "4.attention.v_lin.weight False\n",
            "4.attention.v_lin.bias False\n",
            "4.attention.out_lin.weight False\n",
            "4.attention.out_lin.bias False\n",
            "4.sa_layer_norm.weight False\n",
            "4.sa_layer_norm.bias False\n",
            "4.ffn.lin1.weight False\n",
            "4.ffn.lin1.bias False\n",
            "4.ffn.lin2.weight False\n",
            "4.ffn.lin2.bias False\n",
            "4.output_layer_norm.weight False\n",
            "4.output_layer_norm.bias False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [290/290 00:29, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.736995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.551421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.524863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.504686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.503830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PredictionOutput(predictions=array([[-0.47031346,  3.5176709 ,  1.0031201 , ..., -0.29815102,\n",
            "        -2.2166636 , -1.7575206 ],\n",
            "       [-0.91213435,  4.6522493 ,  0.12920898, ..., -0.91044736,\n",
            "        -2.0345085 , -1.6052629 ],\n",
            "       [-0.86305004,  4.449284  ,  0.22161375, ..., -0.6610231 ,\n",
            "        -2.0758698 , -1.6845105 ],\n",
            "       ...,\n",
            "       [-0.40037066,  2.4958737 ,  3.6499915 , ..., -0.92709213,\n",
            "        -2.9040177 , -2.4778655 ],\n",
            "       [-0.4378421 ,  2.8242404 ,  3.911037  , ..., -1.0357256 ,\n",
            "        -2.8988397 , -2.6958158 ],\n",
            "       [-0.8423688 ,  4.2201877 ,  0.25866452, ..., -0.5178934 ,\n",
            "        -2.0381544 , -1.570565  ]], dtype=float32), label_ids=array([1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1,\n",
            "       1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2,\n",
            "       2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 0, 2,\n",
            "       2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 4, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1,\n",
            "       2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2,\n",
            "       1, 1, 2, 3, 2, 2, 2, 2, 1, 1, 1, 1, 2, 4, 2, 2, 2, 2, 1, 2, 1, 0,\n",
            "       2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 3, 1, 2,\n",
            "       1, 1, 2, 1, 1, 1, 2, 1, 0, 2, 2, 2, 2, 1, 2, 0, 1, 1, 2, 2, 2, 1,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2,\n",
            "       1, 1, 1, 4, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1,\n",
            "       4, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1,\n",
            "       2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 4, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 2, 1, 0, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2,\n",
            "       1, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 4, 1, 1, 1, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2,\n",
            "       0, 2, 1, 2, 2, 2, 2, 2, 0, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2,\n",
            "       2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
            "       1, 1, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 0, 2, 2, 2,\n",
            "       2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1,\n",
            "       2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2,\n",
            "       2, 1, 2, 2, 3, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2,\n",
            "       1, 2, 1, 1, 2, 2, 1, 1, 2, 4, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       4, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2,\n",
            "       2, 1, 2, 1, 1, 2, 0, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1,\n",
            "       1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 1, 0, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 0,\n",
            "       2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 0, 1, 2, 2, 1, 2,\n",
            "       2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 0, 2, 2, 1, 4, 2, 2, 2, 1, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 4, 2, 1,\n",
            "       1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 0, 2, 1, 2,\n",
            "       1, 2, 0, 3, 1, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 3, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 0, 4, 2, 2, 3, 2, 2, 1, 2, 0, 2, 1, 2, 1, 1,\n",
            "       3, 1, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1]), metrics={'test_loss': 0.515175998210907, 'test_runtime': 0.6239, 'test_samples_per_second': 1570.792, 'test_steps_per_second': 49.688})\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       1.00      0.04      0.08        23\n",
            "       happy       0.82      0.82      0.82       364\n",
            "  no-emotion       0.83      0.90      0.87       571\n",
            "         sad       0.00      0.00      0.00        11\n",
            "    surprise       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.83       980\n",
            "   macro avg       0.53      0.35      0.35       980\n",
            "weighted avg       0.81      0.83      0.81       980\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "# Encode the tweets using the BERT tokenizer\n",
        "\n",
        "# 1. Load Label Encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# 2. Fit the label encoder to the label in our dataset\n",
        "le.fit(train[\"label\"])\n",
        "\n",
        "# 3. Create a new column with encoded labels\n",
        "train[\"encoded_label\"] = le.transform(train[\"label\"])\n",
        "val[\"encoded_label\"] = le.transform(val[\"label\"])\n",
        "test[\"encoded_label\"] = le.transform(test[\"label\"])\n",
        "\n",
        "# Validate the mapping:\n",
        "train.groupby([\"label\", \"encoded_label\"]).aggregate(\"count\")\n",
        "\n",
        "\n",
        "# Create pytorch datasets for for the train, val and test datasets\n",
        "\n",
        "# le.inverse_transform([1,4,6])\n",
        "\n",
        "train_labels = torch.tensor(train[\"encoded_label\"].tolist())\n",
        "val_labels = torch.tensor(val[\"encoded_label\"].tolist())\n",
        "test_labels = torch.tensor(test[\"encoded_label\"].tolist())\n",
        "\n",
        "# text encoding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Encoding a sentence\n",
        "sent = \"The quick brown fox jumped over the lazy dog.\"\n",
        "print(f\"Tokenizer output a dictionary: {tokenizer(sent)}\")\n",
        "\n",
        "# We can also decode ids to vocabulary\n",
        "print(tokenizer.decode([101, 1996, 4248, 2829, 4419, 5598, 2058, 1996, 13971, 3899, 1012, 102]))\n",
        "\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    train[\"tweet\"].tolist(),\n",
        "    padding=True,           # pad all inputs to max length\n",
        "    max_length=24,         # Bert max is 512, we choose 24 for computational efficiency\n",
        "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "train_encodings.keys()\n",
        "\n",
        "print(train_encodings)\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val[\"tweet\"].tolist(),\n",
        "    padding=True,           # pad all inputs to max length\n",
        "    max_length=24,         # Bert max is 512, we choose 24 for computational efficiency\n",
        "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    test[\"tweet\"].tolist(),\n",
        "    padding=True,           # pad all inputs to max length\n",
        "    max_length=24,         # Bert max is 512, we choose 24 for computational efficiency\n",
        "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Define Custom Class for DistilBert Inputs\n",
        "class RelationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, encodings: dict):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        e = {k: v[idx] for k,v in self.encodings.items()}\n",
        "        return e\n",
        "\n",
        "\n",
        "# Update encodings with labels\n",
        "train_encodings[\"labels\"] = train_labels\n",
        "val_encodings[\"labels\"] = val_labels\n",
        "test_encodings[\"labels\"] = test_labels\n",
        "\n",
        "# Generate Datasets\n",
        "train_ds = RelationDataset(train_encodings)\n",
        "val_ds = RelationDataset(val_encodings)\n",
        "test_ds = RelationDataset(test_encodings)\n",
        "\n",
        "\n",
        "print(train_ds[:2])\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=7)\n",
        "\n",
        "\n",
        "\n",
        "# Finetune the distilbert model for 5 epochs\n",
        "\n",
        "# Freeze embeddings\n",
        "for name, param in model.distilbert.embeddings.named_parameters():\n",
        "    param.requires_grad = False\n",
        "    print(name, param.requires_grad)\n",
        "\n",
        "# Freeze layers 1-4\n",
        "freeze_layers = [1,2,3,4]\n",
        "for name, param in model.distilbert.transformer.layer.named_parameters():\n",
        "    if int(name[0]) in freeze_layers:\n",
        "        param.requires_grad = False\n",
        "        print(name, param.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    lr_scheduler_type='cosine',\n",
        "    per_device_train_batch_size = 32,\n",
        "    per_device_eval_batch_size = 32,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Extract predictions from the model's output logits and convert them into the emotion labels.\n",
        "\n",
        "\n",
        "preds = trainer.predict(test_ds)\n",
        "print(preds)\n",
        "\n",
        "\n",
        "preds = le.inverse_transform(np.argmax(preds.predictions, axis=1))\n",
        "print(classification_report(test[\"label\"].tolist(), preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOhvhDOImmH"
      },
      "source": [
        "## Task 5. Model Recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ahX42FIM6r"
      },
      "source": [
        "I would recommend to use DistilBERT model among other as this model has given the highest accuracy score of all ranging to 0.82. The accuracy score obtained after Rules-based model were 0.79 where as accuracy score obtained after machine learning w/POS features were 0.76.\n",
        "\n",
        "Rule based models are a good choice for small, easy data. They execute really fast. But these models are specific to certain tasks and domain and might not perform better for other purposes. While they did give good accuracy but again it always depends upon datasets used.\n",
        "\n",
        "Machine Learning models are not restricted to domains and they perform better when incorporated with labelled data by identifying relationships and patterns. If good data is provided and good pre-processing is carried out they can do very well. Also support POS tagging leading to better performace. Although, because they perform better on labelled data these models are not much flexible and pre-processing needs to be of top quality to obtain good results. Overfitting issues are very common in Machine Learning models.\n",
        "\n",
        "I believe DistilBERT to be the best choice to use especially when doing tasks such as opinion mining or emotion detection. DistilBERT model provides State Of The Art performance and can work well on unsupervised data as well. Although it takes a lot of time eventually making it expensive than other models. Also, to get optimal performance we need to provide huge number of data. It is also more complex than other 2 models. Requires more computational resources than other 2 models but still a good choice. Although for this dataset it only performed well in case of 'no-emotion' and 'happy' data unlike the other two models. But good pre-processing and good data quality can improve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AaBGU4lrLRqD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}